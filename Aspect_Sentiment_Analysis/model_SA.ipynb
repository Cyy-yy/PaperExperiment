{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model_SA.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1EHuBlbMwI8LIlM09tCgTZCgzSrf1JAzD","authorship_tag":"ABX9TyO5qKhHAEnAUzud28wLpZ+A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"kRo7vR90s979"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"OZwppT8PsT0Z","executionInfo":{"status":"ok","timestamp":1652274394947,"user_tz":-480,"elapsed":361,"user":{"displayName":"Eric Chen","userId":"13959141610893764710"}}},"outputs":[],"source":["import os\n","\n","os.chdir(\"/content/drive/MyDrive/Notebooks/PAPER_EXPERIMENT\")"]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","from transformers import AutoModel, AutoTokenizer\n","from transformers.optimization import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import warnings\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"9y4UqvGqfHQi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Creat Dataloader"],"metadata":{"id":"wzz8ATvlifkq"}},{"cell_type":"code","source":["class MyDataset(Dataset):\n","\n","    def __init__(self, \n","                 review_path, \n","                 dim_path, \n","                 word_index_path, \n","                 dim_attention_mask_path,\n","                 sentiment_label_path):\n","        super(MyDataset, self).__init__()\n","        self.review = pd.read_csv(review_path)[\"review_tokenized\"]\n","        self.review = self.review.apply(lambda x: \"\".join(eval(x))).tolist()\n","        self.dim = np.load(dim_path)\n","        self.word_index = np.load(word_index_path)\n","        self.dim_attention_mask = np.load(dim_attention_mask_path)\n","        self.sentiment_label = np.load(sentiment_label_path)\n","    \n","    def __len__(self):\n","        return len(self.review)\n","    \n","    def __getitem__(self, idx):\n","        review = self.review[idx]\n","        dim = self.dim[idx]\n","        word_index = self.word_index[idx]\n","        dim_attention_mask = self.dim_attention_mask[idx]\n","        sentiment_label = self.sentiment_label[idx]\n","        return {\n","            \"review\": review,\n","            \"dim_index\": torch.LongTensor(dim),\n","            \"word_index\": torch.LongTensor(word_index),\n","            \"dim_attention_mask\": torch.LongTensor(dim_attention_mask),\n","            \"sentiment_label\": torch.LongTensor(sentiment_label)\n","        }"],"metadata":{"id":"axr9z_Xbs85x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_DataLoader(dataset, batch_size=8, drop_last=True, shuffle=True):\n","    data_loader = DataLoader(dataset, batch_size, shuffle=shuffle, drop_last=drop_last)\n","    return data_loader"],"metadata":{"id":"BpXdZIFGnC72"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define Weighted Embedding Module"],"metadata":{"id":"SHya8zLHguv9"}},{"cell_type":"code","source":["class WeightedEmbedding(nn.Module):\n","\n","    def __init__(self, word_embedding):\n","        \"\"\"\n","        :param word_embedding: size=(vocab_size, 768)\n","        \"\"\"\n","        super(WeightedEmbedding, self).__init__()\n","        word_embedding = torch.FloatTensor(word_embedding)\n","        self.word_embedding = nn.Embedding.from_pretrained(word_embedding)\n","        self.embedding_size = word_embedding.shape[1]\n","        self.softmax = nn.Softmax(dim=-1)\n","    \n","    def forward(self, ernie_output, word_index):\n","        \"\"\"\n","        :param ernie_output: size=(batch_size, 512, 768)\n","        :param word_index: size=(batch_size, 300, 3)\n","        :return weighted_embedding: size=(batch_size, 300, 768)\n","        Note that the sequence includes [CLS]/[SEP]/[PAD] tokens\n","        \"\"\"\n","        word_index = torch.LongTensor(word_index)\n","        batch_size = word_index.shape[0]\n","        max_seq_len = word_index.shape[1]\n","\n","        weighted_embeddings = torch.zeros([\n","            batch_size, max_seq_len, self.embedding_size\n","        ])\n","        ernie_seq_len = ernie_output.size(1)\n","        for i, review in enumerate(word_index):\n","            for j, word in enumerate(review):\n","                if word[2] < ernie_seq_len:\n","                    if ((word[2] - word[1]) > 1):\n","                        word_embedding = self.word_embedding(word[0]).expand([1, -1])  # size=(1, 768)\n","                        char_embedding = ernie_output[i, word[1]:word[2], :]  # size=(span, 768)\n","                        weights = self.softmax(\n","                            torch.matmul(word_embedding, char_embedding.transpose(0, 1))\n","                        )  # size=(1, span)\n","                        weighted_embedding = torch.matmul(weights, char_embedding)  # size=(1, 768)\n","                    elif ((word[2] - word[1]) == 1):\n","                        weighted_embedding = ernie_output[i, word[1]:word[2], :]\n","                    else:\n","                        weighted_embeddings[i, j:] = self.word_embedding(word[0])\n","                        break\n","                    weighted_embeddings[i, j] = weighted_embedding\n","                else:\n","                    weighted_embeddings[i, j] = self.word_embedding(word[0])\n","        return weighted_embeddings"],"metadata":{"id":"vA0ABFmVjIg_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test the module\n","\n","# torch.manual_seed(42)\n","# embeddings = torch.rand([10, 3])\n","# print(embeddings)\n","# print()\n","# ernie_outputs = torch.rand([1, 12, 3])\n","# print(ernie_outputs)\n","# print()\n","# word_index = np.array([\n","#     [[1, 1, 2],\n","#      [5, 3, 4],\n","#      [6, 4, 6],\n","#      [7, 6, 11],\n","#      [0, 11, 11]]\n","# ])\n","# weighted_embedding_layer = WeightedEmbedding(embeddings, word_index)\n","# weighted_embedding_layer(ernie_outputs)"],"metadata":{"id":"woJjag_n9f0c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define Dimension Attention Module"],"metadata":{"id":"yD7I0CsO7CVo"}},{"cell_type":"code","source":["class DimensionAttention(nn.Module):\n","\n","    def __init__(self, word_embeddings):\n","        \"\"\"\n","        :param word_embeddings: size=(vocab_size, 768)\n","        \"\"\"\n","        super(DimensionAttention, self).__init__()\n","        self.linear = nn.Linear(768, 768)\n","        self.tanh = nn.Tanh()\n","        self.softmax = nn.Softmax(dim=1)\n","        word_embeddings = torch.FloatTensor(word_embeddings)\n","        self.embedding = nn.Embedding.from_pretrained(word_embeddings)\n","    \n","    def forward(self, word_index, dim_index, dim_attention_mask):\n","        \"\"\"\n","        :param word_index: size=(batch_size, 300, 3)\n","        :param dim_index: size=(batch_size, 5)\n","        :param dim_attention_mask: size=(batch_size, 5)\n","        :return attention_dim_embedding: (batch_size, 5, 768)\n","        \"\"\"\n","        batch_size = word_index.shape[0]\n","        num_dim = dim_index.shape[1]\n","        word_index = torch.LongTensor(word_index[:, :, 0])  # size=(batch_size, 300)\n","        dim_index = torch.LongTensor(dim_index)\n","        dim_attention_mask = torch.LongTensor(dim_attention_mask).view(batch_size, -1, 1)  # (batch_size, 5, 1)\n","        word_embedding = self.embedding(word_index)  # size=(batch_size, 300, 768)\n","        dim_embedding = self.embedding(dim_index)  # size=(batch_size, 5, 768)\n","        padding_embedding = self.embedding(torch.LongTensor([0]))  # size=(1, 768)\n","        padding_embedding = padding_embedding.expand(batch_size, num_dim, 768)  # size=(batch_size, 5, 768)\n","        U = self.linear(word_embedding)  # size=(batch_size, 300, 768)\n","        U = self.tanh(U)  # size=(batch_size, 300, 768)\n","        alpha = self.softmax(\n","            torch.bmm(U, dim_embedding.transpose(1, 2))\n","        )  # size=(batch_size, 300, 5)\n","        attention_dim_embedding = torch.bmm(alpha.transpose(1, 2), word_embedding)  # size=(batch_size, 5, 768)\n","        attention_dim_embedding = attention_dim_embedding * (1-dim_attention_mask) + \\\n","                                    padding_embedding * dim_attention_mask  # size=(batch_size, 5, 768)\n","        return attention_dim_embedding"],"metadata":{"id":"XmfDDBrp7p_F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define Top-k Pool Module"],"metadata":{"id":"RdK6kMC-kqvz"}},{"cell_type":"code","source":["class TopKPool(nn.Module):\n","\n","    def __init__(self, k):\n","        super(TopKPool, self).__init__()\n","        self.k = k\n","\n","    def forward(self, x):\n","        \"\"\"\n","        :param x: (batch_size, seq_len, embedding_size)\n","        :return: (batch_size, seq_len)\n","        \"\"\"\n","        output = x.topk(self.k, dim=-1)[0].mean(dim=-1)\n","        return output"],"metadata":{"id":"B9MDy2aTkt52"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define Main Module"],"metadata":{"id":"7Iq57SjafIFu"}},{"cell_type":"code","source":["class MainModule(nn.Module):\n","    \n","    def __init__(self, embeddings):\n","        \"\"\"\n","        :param embeddings: size=(vocab_size, 768), dtype=numpy.array\n","        \"\"\"\n","        super(MainModule, self).__init__()\n","        \n","        self.word_embeddings = torch.FloatTensor(embeddings)\n","        self.embedding = nn.Embedding.from_pretrained(self.word_embeddings, freeze=True)\n","        self.ernie = AutoModel.from_pretrained(\"nghuyong/ernie-1.0\")\n","        self.weighted_embedding = WeightedEmbedding(self.word_embeddings)\n","        self.dim_attention = DimensionAttention(self.word_embeddings)\n","        \n","        self.lstm_dim = embeddings.shape[1] // 2  # must be <int> type\n","        self.bi_lstm = nn.LSTM(\n","            input_size=embeddings.shape[1], hidden_size=self.lstm_dim,\n","            bidirectional=True, batch_first=True\n","        )\n","\n","        self.pool = nn.MaxPool1d(embeddings.shape[1], stride=1)\n","        self.linear = nn.Linear(305, 72)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def init_hidden(self, batch_size):\n","        return (torch.randn(2, batch_size, self.lstm_hid_dim).cuda(),\n","                torch.randn(2, batch_size, self.lstm_hid_dim).cuda())\n","    \n","    def forward(self, input_ids, token_type_ids, attention_mask,\n","                word_index, dim_index, dim_attention_mask):\n","        \"\"\"\n","        :param input_ids / token_type_ids / attention_mask: size=(batch_size, 512)\n","        :param word_index: size=(batch_size, 300, 3)\n","        :param dim_index: size=(batch_size, 5)\n","        :param dim_attention_mask: size=(batch_size, 5)\n","        :return: size=(batch_size, 18, 4)\n","        \"\"\"\n","        batch_size = input_ids.shape[0]\n","\n","        # get ERNIE char embeddings, size = (batch_size, 512, 768)\n","        ernie_output = self.ernie(\n","            input_ids=input_ids, token_type_ids=token_type_ids,\n","            attention_mask=attention_mask\n","        )\n","        ernie_char_embeddings = ernie_output.last_hidden_state\n","        \n","        # get weighted word embeddings based on char-word similarity\n","        # size = (batch_size, 300, 768)\n","        weighted_embeddings = self.weighted_embedding(\n","            ernie_output, word_index\n","        )\n","\n","        # get dimension word embedding based on review-dimension attention mechanism\n","        # size = (batch_size, 5, 768)\n","        att_dim_embeddings = self.dim_attention(\n","            word_index, dim_index, dim_attention_mask\n","        )\n","\n","        # fuse the weighted word embeddings and dimension word embeddings\n","        # if concatenate, size = (batch_size, 305, 768)\n","        fuse_embeddings = torch.concat([weighted_embeddings, att_dim_embeddings], dim=1)\n","\n","        # Bi-LSTM layer, size = (batch_size, 305, 768)\n","        hidden_state = self.init_hidden(batch_size)\n","        lstm_output, hidden_state = self.bi_lstm(fuse_embeddings, hidden_state)\n","        \n","        # max pooling layer, size = (batch_size, 305, 1)\n","        pooled_output = self.pool(lstm_output)\n","\n","        # squeeze, size = (batch_size, 305)\n","        squeeze_output = pooled_output.squeeze(dim=2)\n","\n","        # linear layer, size = (batch_size, 72)\n","        linear_output = self.linear(squeeze_output)\n","\n","        # reshape, size = (batch_size, 18, 4)\n","        linear_output.resize_(batch_size, 18, 4)\n","\n","        # softmax layer, size = (batch_size, 18, 4)\n","        output = self.softmax(linear_output)\n","\n","        return output"],"metadata":{"id":"Q_BHGTOx4leg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define Loss Function"],"metadata":{"id":"5Ha1vd79ojoZ"}},{"cell_type":"code","source":["def loss_func(y_pred, target):\n","    \"\"\"\n","    Loss function for multi-output classification\n","    :param y_pred, target: size=(batch_size, 18, 4)\n","    \"\"\"\n","    target = target.float()\n","    num_samples = target.size(0)\n","    num_dimensions = target.size(1)\n","    log_pred = torch.log(y_pred)\n","    loss = -(target * log_pred).sum() / (num_dimensions*num_samples)\n","    return loss"],"metadata":{"id":"XxzaHatgtBhq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define Evaluate Metric Function"],"metadata":{"id":"vVrHfdqRomru"}},{"cell_type":"code","source":["def metrics(y_pred, y_true):\n","    \"\"\"\n","    Calculate precision, recall and f1-score\n","    :param y_pred: size=(batch_size, 18, 4)\n","    :param y_true: size=(batch_size, 18, 4), one-hot labels\n","    \"\"\"\n","    batch_size = y_pred.size(0)\n","    y_pred, y_true = y_pred.cpu().numpy(), y_true.cpu().numpy()\n","    pred_label = y_pred.argmax(axis=-1)\n","    true_label = np.argwhere(y_true == 1)[:, 2].reshape([batch_size, 18])\n","    precision_lst, recall_lst, f1_lst = [], [], []\n","    for i in range(18):\n","        rtn = classification_report(\n","            true_label[:, i], pred_label[:, i], digits=4, output_dict=True\n","        )\n","        macro_avg = rtn[\"macro avg\"]\n","        precision_lst.append(macro_avg[\"precision\"])\n","        recall_lst.append(macro_avg[\"recall\"])\n","        f1_lst.append(macro_avg[\"f1-score\"])\n","    precision = np.mean(precision_lst)\n","    recall = np.mean(recall_lst)\n","    f1_score = np.mean(f1_lst)\n","    return precision, recall, f1_score"],"metadata":{"id":"Q1E28_pTt1LZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define Training Function"],"metadata":{"id":"aHYxn9biEA8x"}},{"cell_type":"code","source":["def train(train_loader, dev_loader, embeddings, \n","          epochs, lr, warmup_rate):\n","    \"\"\"\n","    Main training function\n","    \"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-1.0\", model_max_length=512)\n","\n","    model = MainModule(embeddings)\n","    model.cuda()\n","    \n","    num_training_steps = epochs * len(train_loader)\n","    num_warmup_steps = int(warmup_rate * num_training_steps)\n","    optimizer = AdamW(model.parameters(), lr=lr)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n","    )\n","\n","    for epoch in range(epochs):\n","\n","        \"\"\"Traning stage\"\"\"\n","\n","        model.train()\n","        print()\n","        print(f\"Running EPOCH {epoch+1}\")\n","        print()\n","        train_loss = []\n","        precision = []\n","        recall = []\n","        f1_score = []\n","\n","        for batch_idx, batch_data in enumerate(tqdm(train_loader)):\n","            optimizer.zero_grad()\n","            word_index, dim_index, dim_attention_mask, sentiment_label = \\\n","                batch_data[\"word_index\"].cuda(), \\\n","                batch_data[\"dim_index\"].cuda(), \\\n","                batch_data[\"dim_attention_mask\"].cuda(), \\\n","                batch_data[\"sentiment_label\"].cuda()\n","            review = batch_data[\"review\"]\n","            token_ids = tokenizer(review, padding=\"max_length\", truncation=True)\n","            token_ids = {\n","                key: torch.LongTensor(value).cuda() for (key, value) in token_ids.items()\n","            }\n","            y_pred = model(\n","                **token_ids, word_index=word_index, dim_index=dim_index, \n","                dim_attention_mask=dim_attention_mask\n","            )\n","            loss = loss_func(y_pred, sentiment_label)\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","            train_loss.append(loss.item())\n","            p, r, f1 = metrics(y_pred, sentiment_label)\n","            precision.append(p)\n","            recall.append(r)\n","            f1_score.append(f1)\n","        \n","        epoch_loss = np.mean(train_loss)\n","        epoch_p = np.mean(precision)\n","        epoch_r = np.mean(recall)\n","        epoch_f1 = np.mean(f1_score)\n","\n","        print(f\"EPOCH {epoch+1} Train End : avg_loss = {epoch_loss:.4f}\")\n","        print(f\"precision: {epoch_p:.4f}, recall: {epoch_r:.4f}, f1-score:{epoch_f1:.4f}\")\n","        print()\n","\n","        \"\"\"Evaluation Stage\"\"\"\n","        model.eval()\n","        val_loss = []\n","        val_precision = []\n","        val_recall = []\n","        val_f1 = []\n","        \n","        for batch_idx, batch_data in enumerate(tqdm(dev_loader)):\n","            word_index, dim_index, dim_attention_mask, sentiment_label = \\\n","                batch_data[\"word_index\"].cuda(), \\\n","                batch_data[\"dim_index\"].cuda(), \\\n","                batch_data[\"dim_attention_mask\"].cuda(), \\\n","                batch_data[\"sentiment_label\"].cuda()\n","            review = batch_data[\"review\"]\n","            token_ids = tokenizer(review, padding=\"max_length\", truncation=True)\n","            token_ids = {\n","                key: torch.LongTensor(value).cuda() for (key, value) in token_ids.items()\n","            }\n","            with torch.no_grad():\n","                y_pred = model(\n","                    **token_ids, word_index=word_index, dim_index=dim_index, \n","                    dim_attention_mask=dim_attention_mask\n","                )\n","                loss = loss_func(y_pred, sentiment_label)\n","                p, r, f1 = metrics(y_pred, sentiment_label)\n","            val_loss.append(loss.item())\n","            val_precision.append(p)\n","            val_recall.append(r)\n","            val_f1.append(f1)\n","        \n","        epoch_val_loss = np.mean(val_loss)\n","        epoch_val_p = np.mean(val_precision)\n","        epoch_val_r = np.mean(val_recall)\n","        epoch_val_f1 = np.mean(val_f1)\n","\n","        print(f\"EPOCH {epoch+1} Evaluation End : avg_loss = {epoch_val_loss:.4f}\")\n","        print(f\"precision: {epoch_val_p:.4f}, recall: {epoch_val_r:.4f}, f1-score:{epoch_val_f1:.4f}\")\n","        print()\n","        print(\"=\"*60)"],"metadata":{"id":"8CyCC37frY4O"},"execution_count":null,"outputs":[]}]}