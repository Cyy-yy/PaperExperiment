# 1. 绪论

## 1.1 研究背景及意义

据中国互联网信息中心统计，截止2020年底我国的网民规模达到了10.32亿人，互联网普及率达到73%。互联网的快速发展对人们的生活习惯也带来了各种各样的变化，网民抒发情感的渠道不再局限于线下，越来越多的用户选择在线发布自己对某个事物的评论来表达自己的意见或态度。而在餐饮行业，大量的餐饮消费在线评论文本也由此产生。对于消费者而言，发布的在线评论能够为其他消费者提供决策辅助，有些商家还会对发布优质评论的消费者提供优惠券等奖励，以此推动更多消费者积极发表评论；而对于商家而言，正面的在线评论能够提高自身的知名度和客流量，而负面的在线评论虽然会影响商家的排名，但也从侧面为商家提供了整改方向。当下，线上用户评论文本正呈现指数级增长的趋势，单纯依靠人工的方法对文本蕴含的情感信息进行挖掘存在耗时长且效率低下的问题。因此，互联网企业和相关研究人员开始关注如何利用算法自动化地提取评论文本中包含的有价值的信息，期间也催生了许多相关的技术方法。

情感分析（Sentiment Analysis, SA）是自然语言处理（Natural Language Processing, NLP）的重要研究方向之一，其主要任务是对文本数据中所包含的对某一客观事物的主观情感信息进行挖掘，也称为观点挖掘（Opinion Mining）。根据分析层次的不同，可将情感分析分为粗粒度情感分析和细粒度情感分析两大类，而方面级情感分析（Aspect-based Sentiment Analysis, ABSA）是细粒度情感分析的一个研究方向之一。根据文本中是否明确提到方面项，又可将ABSA任务分为基于方面项的情感分析（Aspect Term Sentiment Analysis,  ATSA）和基于方面类别的情感分析（Aspect Category Sentiment Analysis, ACSA）[35]。餐饮消费在线评论一般包含有多个评价维度（即方面类别），且涉及的维度不一定显式地包含在文本中。例如，对评论“这家饭店太偏远了，但饭菜还不错，卫生条件也可以”来说，就包含了消费者对于该饭店的地理位置、食物口味和饭店环境三个维度的评价。其中对于位置的评价是消极的，而对于口味和环境的评价则是积极的。因此对于餐饮消费评论而言，更适合对其进行ACSA任务。

随着互联网的发展和生活水平的提升，简单地将评论文本划分为单个极性的粗粒度情感分析对评论文本的信息挖掘不够深入，消费者和商家也都希望对产品或服务的各个方面有更全面的了解，因此粗粒度的情感分析不再能满足人们对文本数据挖掘的需求。方面级的情感分析可以对用户在线发布的评论作细粒度的情感分类，对海量的文本作结构化聚合，并在商家的线上UI界面进行展示。一方面，消费者可以据此了解到商家的产品在不同方面存在的优点与不足，进而作出消费决策；另一方面，商家可以通过评论的细粒度情感极性对产品各方面的优劣有更清楚的认知，进而有针对性地对产品进行整改，为消费者提供更优质的产品，进而增加经营利润。因此，将细粒度情感分析任务应用到在线评论中具有重要的研究意义。

## 1.2 国内外研究综述

### 1.2.1 多标签文本分类

文本分类是自然语言处理的经典任务之一，自上世纪50年代末开始就有针对该领域算法的研究[1]。传统的文本分类任务中，每个文本对应一个单独的类别标签，称为单标签文本分类（Single-Label Text Classification）。随着经济社会的发展，人们对待事物的观点和态度越来越多样化，使得文本内容日渐丰富，对文本分类领域的研究也从单标签文本分类扩展到了多标签文本分类（Multi-Label Text Classification）。在多标签文本分类任务中，单个文本可能对应一个或多个类别标签。因此，识别评论文本中所涉及的评价维度本质上是多标签文本分类任务。对多标签文本分类方法的研究大致可以分为两类：基于机器学习和基于深度学习的方法。

#### 1.2.1.1 基于机器学习的方法

从解决问题的策略上看，较为简单的方法是把多标签分类任务看作多个二分类任务的组合，而二分类任务已经有较多成熟的解决方案。Boutell等（2004）[2]提出为每个标签构建一个单独的分类器来解决多标签分类的问题，该方法也称为二元关联法（Binary Relevance, BR）。BR方法思路简单，但该方法潜在假定了标签之间是相互独立的，显然与实际应用场景存在一定差异。Tsoumakas等（2007）[3]通过分析标签集的分布，将具有相同标签集的样本划分为同一类（Label Powerset, LP），进而将多标签分类任务转化为多类别分类（Multi-Class Classification）任务。但该方法也存在明显的缺陷，即在标签数量较多时会导致数据稀疏性问题。针对BR方法忽略了标签之间相关性的问题，Read等（2011）[4]提出分类器链（Classification Chain, CC）的概念，将多个二分类器串联成链式结构，每一个分类器的输入都依赖于上一个分类器的输出，依次调用完成多标签分类任务。

将多标签分类任务视为多个二分类任务的问题转化思想，是为了让数据适应现有的算法，反之也可以通过对现有算法进行改进来适应数据，这种思想称为算法自适应方法。Clare等（2001）[5]等人通过对C4.5决策树算法进行改进，通过修改信息熵目标函数，提出适用于多标签分类任务的ML-DT（Multi-Label Decision Tree）模型。Elisseeff等（2001）[6]对传统的支持向量机（Support Vector Machine, SVM）模型进行改进，提出类似学习系统的排名支持向量机（Rank-SVM）模型用于多标签分类任务，并用核技巧（Kernel Trick）处理非线性情况下的分类任务。Zhang等（2007）[7]提出ML-KNN（Multi-Label K-Nearest Neighbor）模型，该模型借鉴传统KNN的思想，对于未观测样本先统计其最近邻样本的标签信息，而后基于最大后验（Maximum A Posterior, MAP）准则判断未观测样本的标签集。

#### 1.2.1.2 基于深度学习的方法

神经网络是一种在结构和功能上模仿生物神经结构的数学模型或算法，通过构造不同拓扑结构的神经网络能够对任意函数进行逼近。Zhang等（2006）[8]首次将神经网络应用到多标签文本分类任务，提出一种适用于多标签文本分类的BP-MLL（Backpropagation Multi-Label Learning）算法，该算法采用了新的损失函数用于捕捉多标签特征，并在基因功能分类和多标签文本分类任务中取得较优的效果。Nam等（2014）[9]改进了BP-MLL算法，用交叉熵（Cross Entropy）损失函数替代了原有的排序损失函数，并在训练过程中采用AdaGrad优化器、Dropout正则化方法和ReLUs激活函数，该方法在六个大规模多标签文本分类数据集上实现SOTA（State-Of-The-Art）性能，同时也验证了交叉熵损失函数在多标签文本分类任务中的有效性。

然而，简单结构的神经网络在解决NLP任务时也存在着缺陷，一方面其无法保留文本完整的语义信息，另一方面其并未考虑到文本中单词的顺序。随着计算机硬件水平和算力的提升，基于深层次、复杂结构的神经网络方法即深度学习方法开始应用于NLP领域，许多方法在文本分类任务中也取得了有效成果。Berger等（2015）[10]分别使用textCNN[11]和门控循环单元（Gate Recurrent Unit, GRU）对Word2Vec[12]生成的词向量进行特征提取，最后根据人为设定的阈值判断标签类别。对于大规模多标签文本数据集而言，标签类别数过多造成的数据稀疏性是影响分类效果的一大问题，对此Liu等（2017）[13]提出XML-CNN（Extreme Multi-Label CNN）模型。该模型在textCNN的基础上作了如下改进：一是使用动态最大池化（Dynamic Max-Pooling）代替原有最大池化，避免语义信息丢失的问题；二是使用交叉熵损失函数代替原有损失函数；三是在池化层和输出层之间加入全连接层，降低算法的复杂度。实验表明，该模型能够有效提升标签类别数较多的文本分类效果。CNN独有的卷积核使其能够从局部到整体提取文本特征，但词语的顺序也是影响文本语义的关键要素，而CNN却未能对词序建模，在解决文本分类任务存在着局限性。循环神经网络（Recurrent Neural Network, RNN）相较于CNN，在结构上更适用于处理序列数据。Chen等（2017）[14]将CNN和RNN串联，将CNN提取到的文本特征再输入RNN层，相较于单个CNN层的模型在多标签文本分类任务上的效果得到提升。为了使得模型能够考虑到标签之间的依赖关系，又有学者将多标签文本分类任务转化为序列生成任务（Sequence Generation），序列生成模型也开始被应用于该领域。Nam等（2017）[15]提出利用基于RNN的Seq2Seq模型进行多标签文本分类，在编码器端可以对词序建模，在解码器端又可以利用序列生成的思想对标签之间的关联性建模。在使用Seq2Seq架构进行多标签文本分类时，标签顺序对模型的性能会产生较大的影响。同样的标签集，模型会将不同顺序的集合判为负例。因此为减轻Seq2Seq模型对标签顺序的依赖性，Yang等（2018）在解码端增加了Set-Decoder，提出了可用于多标签文本分类的Seq2Set模型，效果优于Seq2Seq。为了更好地捕获不同单词之间以及标签与单词之间的关系，研究人员开始将注意力机制（Attention）加入到模型结构中。Lin等（2018）[16]基于Seq2Seq架构，提出在编码器端采用多层扩展卷积（Multi-level Dilated Convolution, MDC）和长短期记忆网络（Long Short-Term Memory, LSTM）分别构造高层语义特征和词级别的语义特征，而后将解码器端的输出先后与编码器端MDC和LSTM的输出进行Attention操作（Hybrid Attention），兼顾了各个层级的特征信息，模型在路透社新闻数据集和中文博客数据集的多标签分类任务上取得SOTA结果。Xiao等（2019）[17]提出LSAN（LabelSpecificAttentionNetwork）模型，将基于自注意力机制（Self Attention）生成的词向量和基于Attention生成的标签向量进行自适应融合（Adaptive Fusion），在中英文多标签文本数据集上都取得较好的分类效果。在Attention机制的基础上，Google在2017年提出的Transformer网络架构[18]，对NLP领域产生了重要影响，也由此诞生了许多基于大规模语料的预训练模型。Yarullin等（2020）[19]首次将BERT预训练模型[20]应用到多标签文本分类任务，提出序列生成BERT模型。

### 1.2.2 细粒度情感分析

作为NLP领域的一个细分任务，情感分析一直以来都受到广泛关注。早期的情感分析更多是从篇章级（Document Level）或句子级（Sentence Level）对文本蕴含的情感进行识别，其前提假设是整篇文章或单个句子只表达了一种积极或消极的情感[21]。而随着文本所包含信息的增多，人们对情感分析的要求也逐渐提高，情感分析开始从篇章级和句子级的粗粒度层面向细粒度层面发展。对细粒度情感分析任务的研究可以分为两类：一类是将粗粒度情感分析下的两类（积极/消极）或三类（积极/中性/消极）情感标签进行细化，将粗粒度的情感扩展为细粒度的情绪，如喜悦、愤怒、失望等；另一类是方面级（Aspect Level）的情感分析，即对文本中包含的评价实体或评价维度对应的情感极性进行识别，比如一段针对汽车的评论可能包含外观、内饰、动力、操控、空间等多个评价维度，每个维度所对应的情感类别又不尽相同。本课题拟研究的细粒度情感分析任务属于第二类，即方面级的情感分析。针对细粒度情感分析的研究方法主要有三类：基于情感词典、基于机器学习和基于深度学习的方法。

#### 1.2.2.1 基于情感词典的方法

基于词典的情感分析方法需要先构建情感词典，进而根据词典中对应情感词的极性评分来判断文本的情感极性，因此情感词典的质量对情感分析的效果有很大影响。Hu等（2004）[22]认为同义词具有相同的情感，而反义词的情感极性则相反，因此利用WordNet寻找同义词或反义词对人工构建的种子情感词典进行扩充，进而根据情感词与商品特征词的距离判断评论在特定商品维度下的情感极性。Moghaddam等（2010）[23]基于Epinions大众消费点评网站构建情感词典，对于未知形容词则根据WordNet搜索，寻找Epinions网站已有词的相似形容词，并对最邻近的词的评分进行加权作为未知形容词的估计评分。Cruz等（2013）[24]在情感词抽取中融入领域特征信息，并使用WordNet、PMI算法和SentiWordNet进行情感分类。贾闻俊等（2016）在抽取评论文本属性词基础上，采用LDA主题模型抽取情感词，进而基于情感词典判断特定属性下的情感极性。

#### 1.2.2.2 基于机器学习的方法

基于机器学习的情感分析方法需要利用特征工程提取文本数据相关特征，而后再用机器学习算法进行分类。Boiy等（2009）[25]通过人工提取文本特征，进而采用SVM、多项式朴素贝叶斯（Multinomial Naive Bayes, MNB）以及最大熵（Maximum Entropy）模型三种传统机器学习方法，在多语言文本数据集的细粒度情感分析任务上取得有效成果。Jiang等（2011）[26]针对Twitter数据集构造了目标独立（Target-independent）和目标相关（Target-dependent）两类特征，其中目标独立的特征包括文本中的单词、标点符号、表情符号以及基于情感词典的特征，目标相关的特征则根据句法依存关系提取。最后基于SVM对Twitter文本在给定查询（query）向量下的情感极性进行分类，此处的query向量等同于方面级情感分析下的aspect-term。Wagner等（2014）[27]通过提取N-gram特征，加入基于情感词典以及方面项和情感词的距离计算得到的情感得分，采用SVM对餐饮和笔记本领域的评论数据集进行细粒度情感分析，取得了较优的分类效果。

#### 1.2.2.3 基于深度学习的方法

随着深度学习方法的发展，越来越多的研究着眼于将深度学习模型应用于情感分析领域，由此也衍生了许多针对情感分析任务的深度学习算法。对于细粒度情感分析任务，人们一般会根据目标词附近的上下文来判断该目标的情感类别。基于这一观点，Tang等（2015）[28]提出目标依赖的LSTM模型（Target-Dependent LSTM, TD-LSTM），模型在目标词处将句子断开，进而用两个LSTM网络从正反两个方向作特征提取并进行拼接。但TD-LSTM模型并没有充分考虑目标词与评论文本的关系，因此Tang等（2015）[28]又提出目标连接的LSTM模型（Target-Connection LSTM, TC-LSTM）。该模型在TD-LSTM的基础上，在输入层将目标词向量与评论文本词向量进行拼接融合，随后由两个LSTM网络提取特征。实验显示，TD-LSTM和TC-LSTM相较于LSTM模型，在方面级的细粒度情感分析任务中取得更高的准确率，而TC-LSTM对目标词与评论文本的融合也提升了分类的效果。为了更好地捕捉方面项和评论文本之间的关联性，Wang等（2016）[29]提出将Attention机制引入方面级情感分析中，提出AT-LSTM（Attention-based LSTM）和ATAE-LSTM（Attention-based LSTM with Aspect Embedding）模型。AT-LSTM模型将LSTM提取的特征向量和方面向量进行拼接，然后采用Attention机制使模型重点关注对方面情感极性判断有较大影响的特征。而ATAE-LSTM在AT-LSTM的基础上，在输入层加入了方面词向量，进一步加强方面项与评论文本之间的关系。实验结果也显示，加入Attention机制的模型相较于TD-LSTM、TC-LSTM效果更好。MA等（2017）[30]认为方面项与其上下文的关联性应该是相互的，因此提出IAN（Interactive Attention Networks）模型，对方面项和上下文采用交互注意力机制提取特征并融合，在方面级情感分析任务中取得优于ATAE-LSTM的效果。随着ELMo[31]、BERT[20]等预训练模型的出现，NLP领域进入了预训练时代，研究人员开始将基于预训练模型的方案应用到情感分析任务中。由于BERT的输入可以是句子对，SUN等（2019）[32]基于方面项构造了辅助句子，与评论文本形成句子对作为BERT的输入，将ABSA任务转化为类似问答或推断的句子对分类任务。作者采用了四种不同方式构造辅助句子，并对BERT进行微调，在两个细粒度情感分类数据集上实现SOTA效果。Jiang等（2019）[33]将胶囊网络（Capsule Network, CapsNet）[34]与BERT结合，提出CapsNet-BERT模型，既利用了CapsNet捕捉方面项和评论文本之间的关系，又利用了BERT在大规模语料上学到的语义信息。

对现有文献进行梳理发现：

（1）对于多标签文本分类任务，早期的研究都是基于机器学习方法，解决问题的策略主要有两种：一是使数据适应算法，将多分类转换为二分类问题；二是使算法适应数据，即对现有算法进行改造。但机器学习方法的分类效果很大程度上取决于特征工程的质量，而传统的机器学习方法往往需要人工提取特征，因此特征工程的质量得不到有效保障。而基于深度学习的方法能够自动提取特征，省去了复杂的特征工程环节，也更适合当下数据规模日益增大的应用场景。因此，本课题拟在LSAN模型结构的基础上进行改进，将其应用在评论文本中评价维度的识别任务中。

（2）对于细粒度情感分析任务，早期的研究是基于情感词典计算得分的方法进行情感分类，该方法思路简单，但领域适用性较差。对不同领域的评论文本，需要单独构建针对该领域的情感词典才能取得比较好的分类效果。而基于机器学习的方法在情感分析任务中的准确率较基于情感词典的方法有所提升，但分类效果同样也受人工提取特征的质量的影响。随着深度学习算法的发展，越来越多的深度学习模型被应用到细粒度情感分类任务中，也取得了比传统方法更好的效果。但是，当前大多数深度学习模型都是在英文数据集上实现SOTA性能，专门针对中文数据集的细粒度情感分析模型相对较少。而在预训练模型的应用上，大部分研究都是利用预训练模型输出的字向量建模，而对于中文而言，词语所表达的语义信息要比单个字所表达的信息更有意义。因此，本课题将在评价维度识别任务的基础上，对百度ERNIE预训练模型生成的字向量加权构造词向量，并将评价维度信息融合到评论文本的语义特征中完成基于方面类别的细粒度情感分析任务。

## 1.3 研究思路及方法

### 1.3.1 数据来源

本研究拟采用的数据集是来自美团在2021年4月开源的ASAP（Aspect Category Sentiment Analysis and Rating Prediction）数据集，该数据集收录了美团旗下大众点评平台的合计46730条餐饮消费在线评论，经过人工判断标注细粒度情感极性，是目前基于方面类别的细粒度情感分析领域规模最大的中文数据集。该数据集在收录过程中遵从以下几个规则：①只收录字符数在50~1000之间的评论文本，不考虑较为极端的超短或超长文本；②不考虑非中文字符占比超70%的评论文本；③过滤了广告等废文。这些规则保证了后续研究能够获得较高质量的数据。

### 1.3.2 数据预处理

由于数据集为网络文本，可能存在HTML标签、Unicode乱码、表情等，标签或乱码并不具备语义信息，预处理阶段考虑删除。而表情虽然表达了情感信息，但在数据集中也是以Unicode编码存在，因此在构建模型输入时也不考虑表情符号。不同用户对输入法的使用习惯可能存在差异，导致评论文本中可能存在简体和繁体两种中文字体，而简繁体并不影响文字的语义信息，因此预处理时应将评论文本中的中文字体统一为简体，避免后续同一个汉字出现两种不同语义向量的情况。经过乱码处理、简繁转换之后，得到的是一份相对干净的文本数据集。而后便需要对文本进行分词，为词向量的训练构建输入数据。现有的分词工具都是基于其自带的字典，采用动态规划的方法进行分析，属于通用版本的分词，针对特定领域的分词可能存在误差。考虑到本研究采用的数据集为餐饮消费领域，因此可以对分词工具原有词典进行扩充，增加搜狗饮食分类网络词库，提高餐饮领域评论文本分词的准确率。最后在预处理阶段，还需要去除文本中的标点符号以及停用词，保留评论文本主要信息。

### 1.3.3 基于LSAN-CNN的评价维度识别

对评论文本的细粒度情感分析，其目的是为了对评论中所涉及评价维度的情感极性进行分类，因此本研究拟将细粒度情感分析任务分为两个阶段完成。第一步是对评论文本中涉及的评价维度进行识别，第二步将评论文本与第一步中的评价维度识别结果进行融合，预测不同评价维度下的情感极性。

评价维度本质上属于多标签文本分类任务，该领域主流的研究方法都是基于深度学习算法，因此本研究拟在LSAN模型的基础上进行改进，对文本涉及的评价维度进行预测。LSAN模型主要通过Self-Attention和Label-Attention机制分别生成两个维度相同的文本表征矩阵，进而采用自适应融合的方式对两个矩阵进行加权，得到最终分类器的输入。在文本特征提取阶段，LSAN采用的是Bi-LSTM，该方法可对文本的词序建模。在此基础上，本研究拟采用textCNN进一步提取文本的n-gram特征，得到文本的表征向量，与基于Self-Attention和Label-Attention得到的表征向量进行融合，最后输入分类器。

### 1.3.4 基于ERNIE的方面级情感极性分类

对于情感极性分类任务，本研究拟采用预训练模型ERNIE。中文版本的预训练模型得到的是文本的字向量，以往的研究大部分也直接采用预训练模型产生的字向量作为文本的表征。但一个完整的中文词汇往往比单个汉字具备更丰富的语义信息，基于这一前提，本研究将基于字和词的相似度，对字向量加权，构建词向量，进而与第一个任务中识别到的评论维度信息进行融合，作为最终情感极性分类器的输入。

模型对比方面，本研究拟将基于字词相似度加权的方法与直接采用字向量建模的方法进行比较，采用macro-F1作为衡量模型性能的指标。

## 1.4 章节安排

本研究内容拟分为六个章节，各个章节安排如下：

第一章：绪论。首先对本研究的背景、目的及意义进行阐述，接着对细粒度情感分析相关的国内外研究现状进行梳理，对现有研究中存在的问题进行分析，进而给出本研究拟采用的研究方法、创新点以及内容结构安排。

第二章：相关概念及分析方法。本章节主要介绍细粒度情感分析的相关概念、模型理论和分析方法，包括当前常用的基于深度学习的词向量训练方法、自然语言处理领域常用的循环神经网络和卷积神经网络，并对注意力机制的计算方法以及基于Transformer架构的预训练模型进行介绍。

第三章：数据来源及预处理。主要介绍模型所采用的数据集来源，由于数据集来源网络开源数据，因此数据的质量也需要有保障。本章将简要介绍官方披露的数据集采集过程中的一些细节问题，以证明该数据集具备较高的质量，能够为后续研究提供可靠保障。在此基础上，对数据集作必要的预处理，包括分词、去乱码、去停用词等步骤。

第四章：基于LSAN-CNN模型的评论维度识别。本章主要介绍细粒度情感分析的第一个任务——评价维度识别，首先介绍用于多标签文本分类的LSAN模型的基本结构，对模型存在的不足进行分析，并给出本研究在LSAN模型基础上的优化思路，最后通过实验比较改进模型与原模型在评价识别任务中的效果验证该改进模型的有效性。

第五章：基于ERNIE的方面级情感极性分类。本章在评价维度识别任务的基础之上，进一步采用预训练的ERNIE模型并融合评价维度信息对评论的细粒度情感极性做分类。其中，将详细介绍本研究提出的基于字词相似度加权的词向量构造方法，并通过实验对比单纯使用预训练模型产生的字向量做分类的各项性能指标，以验证本研究提出方法的有效性。

第六章：结论及展望。本章将总结研究成果，在展望中分析本研究尚存在的不足之处以及未来的改进方向。

## 1.5 创新点与不足

### 1.5.1 创新点

本研究创新点主要为以下两个方面：

在评价维度识别任务中，本研究在梳理了多标签文本分类领域的研究现状后发现，当前主流的研究方法都是基于深度学习，在大规模中文多标签文本分类任务中，LSAN模型取得了不错的效果。但是该模型在特征提取阶段只采用了Bi-LSTM模型，无法提取到文本的局部特征，因此本研究拟增加textCNN模型，利用其卷积核的特性提取文本的N-gram特征，弥补LSAN模型在特征提取阶段的不足。

在细粒度情感极性分类任务中，梳理现有文献发现当前该领域主流的研究方法同样基于深度学习，而且随着预训练模型的发展，越来越多的研究采用”预训练+微调“的方式解决情感分析任务。基于中文语料的预训练模型针对字向量建模，因此大多数研究都直接采用模型生成的字向量作为下游模型结构的输入。但对于中文而言，词语的语义信息往往要比单个汉字更加丰富，也更有意义。基于此，本研究在运用中文预训练模型的同时提出了字词相似度加权构造词向量的方法，以弥补基于中文预训练模型的情感分析任务中单纯依靠字向量建模的不足。

### 1.5.2 研究不足

尽管本文研究所提出的改进方法能够提升模型在方面级情感分析任务上的效果，但研究过程仍然存在以下不足之处：①在数据预处理阶段，本文研究并未对数据均衡性作探究，标签不均衡可能对模型的识别效果造成一定的影响；②本文研究将方面级情感分析任务分为两阶段完成，该模式下会存在误差传导的问题，即第一阶段误差会影响第二阶段的效果，且本文并未将研究结果与多任务模型做对比。

